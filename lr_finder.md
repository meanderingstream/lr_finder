---
title: Learning Rate Finder
theme: moon
revealOptions:
    transition: 'fade'
---
## Intro

Scott Mueller

###### smueller.tampa.ai@gmail.com

---
## Tampa.ai

Looking for Presenters


---
## A disciplined approach to neural network hyper-parameters: Part 1 â€“ learning rate, batch size, momentum, and weight decay

https://arxiv.org/abs/1803.09820


---
* Super Convergence
* R1: Unreasonable Effectiveness of Validation/Test Loss
* R2: Horizontal Part of Test Loss
* Underfitting
* Overfitting
* Cyclical Learning Rates

---
* R3: Amount of Regularization Must Be Balanced
* R4: Highest Performance Minimizing Computational Time
* R5: Optimial Momentum Improves Network Training
* R6: Weight Decay is Key Knob for Tuning Regularization
* Short Recipe

---
## Super Convergence
<img src="./images/LRvsCLRresnet56.png"  height="500">

---
## One Cycle
<img src="./images/imagenetInceptionSC2.png"  height="500">

---
## R1: THE UNREASONABLE EFFECTIVENESS OF VALIDATION/TEST LOSS
<img src="./images/NormalFatherSon.JPG"  height="500">

---
## R2: Horizontal Part of Test Loss
<img src="./images/FatherSon.jpg"  width="500">

---
## Underfitting
<img src="./images/bears.png"  width="500">

---
## Overfitting

---
## R3: Amount of Regularization Must Be Balanced

---
## R4: Highest Performance Minimizing Computational Time
<img src="./images/zach_zane.jpg"  width="500">

---
## R5: Optimial Momentum Improves Network Training

<img src="./images/RingNeck.jpg"  width="500">

---
## R6: Weight Decay is Key Knob for Tuning Regularization
<img src="./images/ColabNotebook.png"  width="500">

---
## Short Recipe
<img src="./images/making_folders.png"  width="500">

---

[Knowfalls.com](https://knowfalls.com/team.html)

Looking for Founder Engineers

Elixir, Functional Programming, Rails, Experience
